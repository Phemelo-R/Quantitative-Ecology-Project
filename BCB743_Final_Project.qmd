---
title: "BCB743 Final Project"
author: 
  - "Shane Ngwenya"
  - "<a href='https://github.com/Phemelo-R' target='_blank'>Phemelo Rutlokoane</a>"
  - "Kezia Samuels"
date: "18 July 2025"
number-sections: true
format:
  html:
    toc: true
    code-fold: false
    code-line-numbers: true
    highlight-style: github
    css: LesCodex2.css
    self-contained: true
execute:
  echo: true
  warning: false
  message: false
embed-resources: true
---

## Loading Libraries

This section involves loading all the necessary libraries that have been used in this project. They do not necessarily apply to just the first section of this project.

```{r}
#| label: load-packages

library(tidyverse)    # For data manipulation and visualization
library(sf)           # For working with spatial data
library(terra)        # For working with spatial data and rasters
library(geodata)      # For downloading and working with geospatial data
library(FNN)          # For nearest neighbour matching
library(vegan)        # For ecological data analysis
library(readxl)       # For reading Excel files
library(corrplot)     # For visualising correlations
library(ggcorrplot)   # For visualising correlations
library(ggpubr)       # For combining ggplots
library(Hmisc)        # for rcorr 
library(cluster)      # For clustering analysis
library(readr)        # For reading CSV files
library(ggrepel)      # For better text labels in ggplot
library(viridis)      # For colour palettes
library(usdm)         # For variance inflation factor (VIF) calculations
library(tibble)       # For tibbles
library(colorspace)   # For colour manipulation
library(reshape2)     # For reshaping data
```

------------------------------------------------------------------------

## Data Files used for major data analysis

This section includes only the output files generated throughout the first two steps to be used in the final data analysis. The data files can be found in this github [repository](https://github.com/Phemelo-R/Quantitative-Ecology-Project).

------------------------------------------------------------------------

## Biological Data acquisition and processing - *Shane Ngwenya*

### Defining study site extent with centroids

The first step involves making a 0.25° × 0.25° (15′ × 15′) latitude–longitude grid for South Africa, which will be our study area for the project.

```{r}
#| label: create-grids
#| eval: false

# Define bounding box for South Africa
xmin <- 16     # degrees East
xmax <- 33
ymin <- -35    # degrees South
ymax <- -22

# Create grid with 0.25° resolution (quarter-degree)
grid <- st_make_grid(
  cellsize = c(0.25, 0.25),
  offset   = c(xmin, ymin),
  n        = c((xmax - xmin) / 0.25, (ymax - ymin) / 0.25),
  crs      = 4326,        # WGS 84 coordinate reference system
  what     = "polygons"   # Create polygons for each grid cell
)
```

Now that the grids have been created, we now create centroids for each grid cell to extract the coordinates and create sites based off the centroids:

```{r}
#| label: create-centroids
#| eval: false

# Compute centroids of each grid cell
centroids <- st_centroid(grid)

# Convert to data frame and add coordinates and cell_id
centroids_df <- st_as_sf(centroids) %>%
  mutate(cell_id = row_number()) %>%
  mutate(
    lon_centroid = st_coordinates(.)[,1],
    lat_centroid = st_coordinates(.)[,2]
  ) %>%
  st_drop_geometry()

# 5. Export centroids as CSV
write.csv(
  centroids_df,
  "SA Quarter Degree Centroids.csv",
  row.names = FALSE
)
```

------------------------------------------------------------------------

### Collecting biome, bioregion, and vegetation data

This step involved extracting biome data from the South African Vegetation Map. However, in order to do that, we first had to convert our centroid points into sf point layers

```{r}
#| label: veg-data
#| eval: false

# Read your centroids CSV
centroids <- read.csv("SA Quarter Degree Centroids.csv")

# Convert to an sf POINT layer (WGS84)
pts <- st_as_sf(
  centroids,
  coords = c("lon_centroid","lat_centroid"),
  crs    = 4326,
  remove = FALSE
)

# Read the VegMap shapefile (native CRS)
veg <- st_read("QGIS/Shapefile/NVM2024Final_IEM5_12_07012025.shp", quiet = TRUE)

# Repair invalid polygons:
# Try sf’s built-in make_valid (requires GEOS ≥ 3.6):
veg <- st_make_valid(veg)

# If that still errors, you can fall back to the “buffer zero” hack:
# veg <- st_buffer(veg, 0)

# Reproject the cleaned VegMap to match pts (EPSG:4326)
veg <- st_transform(veg, st_crs(pts))

# Select only the vegetation fields you need
veg_sel <- veg %>%
  dplyr::select(
    biome     = T_BIOME,
    bioregion = T_BIOREGIO,
    veg_type  = T_Name
  )

# Spatial join: attach veg attributes to each centroid
pts_veg <- st_join(
  pts,
  veg_sel,
  join = st_intersects,
  left = TRUE
)

# Drop geometry and keep just the columns you want
out <- pts_veg %>%
  st_drop_geometry() %>%
  dplyr::select(
    cell_id,
    lon_centroid,
    lat_centroid,
    biome,
    bioregion,
    veg_type
  )

# Write to CSV
write.csv(
  out,
  "centroids_with_vegtype.csv",
  row.names = FALSE
)
```

------------------------------------------------------------------------

## Environmental data sourcing and extraction - *Phemelo Rutlokoane*

### Loading all the environmental rasters required

In this section we are going to load all the raster data we downloaded from WorldClim as well as the centroid points we made in the first section. I also added the NDVI raster I clipped from QGIS. All the raster files loaded in this section were projected in QGIS to EPSG:4326 (WGS 84) so that the crs matches that of the biological data and centroids.

```{r}
#| label: env-data

# Load grid centroids
points <- read.csv("Project_workplan/SA Quarter Degree Centroids.csv")

# Load environmental rasters

# Mean Annual precipitation
MeanPrecip_raster <- rast("Project_workplan/SA_Precip/SA_MeanPrecip.tif") 

# Precipitation of driest quarter
DryPrecip_raster <- rast("Project_workplan/SA_Precip/SA_Precip_DryQ.tif")

# Precipitation of wettest quarter
WetPrecip_raster <- rast("Project_workplan/SA_Precip/SA_Precip_WetQ.tif")  

# Precipitation Seasonality (Coefficient of Variation)
SeaPrecip_raster <- rast("Project_workplan/SA_Precip/SA_Precip_Seasonality.tif") 
# Elevation of South Africa
Elevation_raster <- rast("Project_workplan/SA_Elev/SA_Elev.tif")

# Mean Annual Temperature
MeanTemp_raster <- rast("Project_workplan/SA_Temp/SA_Mean_Temp.tif")

# Maximum Temperature of warmest month
HotTemp_raster <- rast("Project_workplan/SA_Temp/SA_MaxTemp.tif") 

# Minimum Temperature of coldest month
ColdTemp_raster <- rast("Project_workplan/SA_Temp/SA_MinTemp.tif")  

# Mean Diurnal Range (Mean of monthly (max temp - min temp))
DiurnalTemp_raster <- rast("Project_workplan/SA_Temp/Mean_Diurnal_RangeTemp.tif") 

# Normalised Difference Vegetation Index
NDVI_raster_raw <- rast("Project_workplan/SA_NDVI/SA_NDVI_merge.tif")     
```

------------------------------------------------------------------------

### Calculate true NDVI by applying scale factor and offset

```{r}
#| label: scale-fact
#| results: hide

# Inspect the NDVI metadata
meta <- terra::sources(NDVI_raster_raw)  # Extract metadata

# Get raw metadata
terra::describe("Project_workplan/SA_NDVI/SA_NDVI_merge.tif")

# Apply scale factor
NDVI_raster <- NDVI_raster_raw * 0.00000001

# Verify range after scaling
global(NDVI_raster, range, na.rm = TRUE) # Should be between -1 and +1

# Mask invalid pixels
NDVI_raster[NDVI_raster < -1 | NDVI_raster > 1] <- NA
```

------------------------------------------------------------------------

### Extract raster values for each centroid

In order to accomplish this, we first have to convert our centroids into sf objects that will be able to extract the values of the environmental rasters properly and accurately.

```{r}
#| label: extract-rast-val

# Convert to sf object
points_sf <- st_as_sf(points, coords = c("lon_centroid", "lat_centroid"), crs = 4326, remove = FALSE)

# Extract mean annual precipitation value for each point
MeanPrecip <- terra::extract(MeanPrecip_raster, vect(points_sf))

# Extract precipitation of driest quarter value for each point
DryPrecip <- terra::extract(DryPrecip_raster, vect(points_sf))

# Extract precipitation of wettest quarter value for each point
WetPrecip <- terra::extract(WetPrecip_raster, vect(points_sf))

# Extract precipitation seasonality (coefficient of variation value for each point
SeasonPrecip <- terra::extract(SeaPrecip_raster, vect(points_sf))

# Extract elevation value for each point
Elevation <- terra::extract(Elevation_raster, vect(points_sf))

# Extract NDVI value for each point
NDVI <- terra::extract(NDVI_raster, vect(points_sf))

# Extract mean annual temperature value for each point
MeanTemp <- terra::extract(MeanTemp_raster, vect(points_sf))

# Extract maximum temperature for hottest month value for each point
MaxTemp <- terra::extract(HotTemp_raster, vect(points_sf))

# Extract minimum temperature for coldest month value for each point
MinTemp <- terra::extract(ColdTemp_raster, vect(points_sf))

# Extract mean diurnal range value for each point
DiurnalTemp <- terra::extract(DiurnalTemp_raster, vect(points_sf))
```

### Cleaning the data and making it neat

```{r}
#| label: clean-data

# Join data into one table 
Environmental_variables <- cbind(points_sf, MeanPrecip, DryPrecip, WetPrecip, SeasonPrecip, Elevation, NDVI, MeanTemp, MaxTemp, MinTemp, DiurnalTemp)

# Convert environmental_variables from sf into data frame
Environmental_variables <-  as.data.frame(Environmental_variables)

# Select wanted columns
Env_var <- Environmental_variables |> 
  dplyr::select(cell_id, lon_centroid, lat_centroid, SA_MeanPrecip, SA_Precip_DryQ, SA_Precip_WetQ, SA_Precip_Seasonality, SA_Elev, SA_NDVI_merge, SA_Mean_Temp, SA_MaxTemp, SA_MinTemp, Mean_Diurnal_RangeTemp)

# Rename the columns
Env_var <- Env_var |> 
  rename(
    lon            = lon_centroid,
    lat            = lat_centroid,
    MeanPrecip     = SA_MeanPrecip,
    DryPrecip      = SA_Precip_DryQ,
    WetPrecip      = SA_Precip_WetQ,
    SeasonalPrecip = SA_Precip_Seasonality,
    Elevation      = SA_Elev,
    NDVI           = SA_NDVI_merge,
    MeanTemp       = SA_Mean_Temp,
    MaxTemp        = SA_MaxTemp,
    MinTemp        = SA_MinTemp,
    DiurnalTemp    = Mean_Diurnal_RangeTemp
  )

# Arrange cell_id in descending order
Env_var <- Env_var |> 
  arrange(cell_id)

# Remove cell_id "159" and "3394" because they have NA values
Env_var <- Env_var |> 
  filter(!cell_id %in% c("159","3394"))

# Save the table as a .csv file
write.csv(Env_var, "Project_workplan/Env_var.csv", row.names = FALSE)
```

------------------------------------------------------------------------

### Adding and preparing the land cover data

Prior to adding the dataset, we had to clip the quarter grid and land cover raster to South Africa's shape file. From then we projected them to EPSG:4326 (WGS 84) again so that we have consistent scaling. `Landcover.csv` is the output file from QGIS's zonal histogram function.

```{r}
#| label: land-cover

# Load quarter-degree grid (as polygons)
grid <- vect("Project_workplan/SA_quart_grid/Clipped_Quarter_Grid.shp")

# Load land cover raster
lc_raster <- rast("Project_workplan/SA_NLC_2020_Geographic.tif.vat/SA_NLC_2020_GEO.tif")

# Read the lookup table
lc_df <- read.csv("Project_workplan/Landcover.csv")

# Create a lookup table for land cover classes
lc_table <- levels(lc_raster)

# save the land cover classes to a CSV file
write.csv(lc_table, "landcover_classes.csv", row.names = FALSE)
```

### Grouping the land cover by classification group

```{r}
#| label: group-land-cover

# Load the land cover classes CSV
lc_classes <- read.csv("landcover_classes.csv")

# Initialize vector with default NA or "Other"
lc_classes$SALCC1 <- NA_character_

# Assign groups by rows/Values
lc_classes$SALCC1[2:8] <- "Forested land"
lc_classes$SALCC1[9:12] <- "Shrubland"
lc_classes$SALCC1[13:14] <- "Grassland"
lc_classes$SALCC1[15:22] <- "Waterbodies"
lc_classes$SALCC1[23:25] <- "Wetlands"
lc_classes$SALCC1[26:32] <- "Barren Land"
lc_classes$SALCC1[33:47] <- "Cultivated"
lc_classes$SALCC1[48:68] <- "Built-up"
lc_classes$SALCC1[69:73] <- "Mines & Quarries"
lc_classes$SALCC1[74] <- "Cultivated"

write.csv(lc_classes, "landcover_classes_grouped.csv", row.names = FALSE)
```

### Pre-process for merging to environmental data

```{r}
#| label: pre-process

# Load the new NLC classes CSV
nlc_classes <- read.csv("landcover_classes_grouped.csv")

# This grabs all columns that start with "HISTO_"
hist_cols <- grep("^HISTO_", names(lc_df), value = TRUE)

# Extract raster value from histogram column names
hist_values <- as.integer(sub("HISTO_", "", hist_cols))

# Match those to the SALCC1 column by Value
class_map <- data.frame(
  hist_col = hist_cols,
  value = hist_values,
  class_group = lc_classes$SALCC1[match(hist_values, lc_classes$Value)]
)
```

### Sum histogram by class group

```{r}
#| label: sum-histo

# Extract just the histogram counts
hist_counts <- lc_df[, hist_cols]

# Assign names to histogram columns as their class groups
names(hist_counts) <- class_map$class_group

# convert to matrix
hist_counts_mat <- as.matrix(hist_counts)

groups <- class_map$class_group
unique_groups <- unique(groups)

# Create a list with indices for each group
group_indices <- lapply(unique_groups, function(g) which(groups == g))
names(group_indices) <- unique_groups

hist_summarised <- do.call(cbind, lapply(group_indices, function(idxs) {
  rowSums(hist_counts_mat[, idxs, drop = FALSE], na.rm = TRUE)
}))

hist_summarised <- as.data.frame(hist_summarised)

# Add cell_id back
hist_summarised$cell_id <- lc_df$id

# Reorder so cell_id is first column
hist_summarised <- hist_summarised[, c("cell_id", setdiff(names(hist_summarised), "cell_id"))]
```

### Add coordinates to land cover data

```{r}
#| label: add-coords
#| results: hide

# Load your grid shapefile
grid_sf <- st_read("Project_workplan/SA_quart_grid/Clipped_Quarter_Grid.shp")

# Filter polygons corresponding to your hist_summarised cell_id
grid_sel <- grid_sf[grid_sf$id %in% hist_summarised$cell_id, ]

# Extract centroids of these polygons
centroids <- st_centroid(grid_sel)

# Add centroid coords to hist_summarised by matching cell_id
centroid_coords <- st_coordinates(centroids)

# Ensure order matches hist_summarised rows
# For safety, order grid_sel by id and hist_summarised by cell_id
grid_sel <- grid_sel[order(grid_sel$id), ]
hist_summarised <- hist_summarised[order(hist_summarised$cell_id), ]

hist_summarised$lon <- centroid_coords[,1]
hist_summarised$lat <- centroid_coords[,2]
```

------------------------------------------------------------------------

### Prepare coordinate matrices for matching

```{r}
#| label: coord-match-prep

# Add env_df with coordinates
env_df <- read.csv("Project_workplan/Env_var.csv")

# Extract coordinates from env_df
env_coords <- as.matrix(env_df[, c("lon", "lat")])

# Extract coordinates from hist_summarised
hist_coords <- as.matrix(hist_summarised[, c("lon", "lat")])
```

### Find nearest neighbour matches

```{r}
#| label: FNN

# For each hist_summarised point, find nearest env_df point index
nn_idx <- get.knnx(env_coords, hist_coords, k = 1)$nn.index[,1]

# Replace or assign IDs in hist_summarised

hist_summarised$cell_id <- env_df$cell_id[nn_idx]

# Restore cell ID
hist_summarised$cell_id <- lc_df$id

hist_summarised$original_cell_id <- lc_df$id  # This is for traceability

# env_coords = matrix of lon/lat from env_df
nn_idx <- get.knnx(env_coords, hist_coords, k = 1)$nn.index[,1]

# Now assign the matched cell_id from env_df to hist_summarised
hist_summarised$cell_id <- env_df$cell_id[nn_idx]

hist_summarised <- hist_summarised |> 
  select(-original_cell_id)  # Remove the temporary column

# Match the rows by removing irrelevant sites

env_coords <- as.matrix(env_df[, c("lon", "lat")])
hist_coords <- as.matrix(hist_summarised[, c("lon", "lat")])

# For each env site, find the index of the nearest hist_summarised site
nn_idx <- get.knnx(hist_coords, env_coords, k = 1)$nn.index[,1]

# Subset and reorder hist_summarised to match env_df
hist_matched <- hist_summarised[nn_idx, ]
```

------------------------------------------------------------------------

### Calculate proportions

```{r}
#| label: calc-prop

# Remove lon, lat, cell_ID from the columns to calculate proportions
lc_cols <- setdiff(names(hist_matched), c("cell_id", "lon", "lat"))

# Total pixels per site
pixel_totals <- rowSums(hist_matched[lc_cols], na.rm = TRUE)

# Convert to proportions
hist_props <- hist_matched
hist_props[lc_cols] <- lapply(hist_props[lc_cols], function(col) col / pixel_totals)

# Check proportions
summary(rowSums(hist_props[lc_cols], na.rm = TRUE))

# Merge it to the environment data frame
env_final <- cbind(env_df, hist_props[, !(names(hist_props) %in% c("lon", "lat", "cell_id"))])

# arrange by cell_ID
env_final <- env_final |> 
  arrange(cell_id)

# Save as csv
write.csv(env_final, "Project_workplan/Num_env_var.csv", row.names = FALSE)
```

------------------------------------------------------------------------

## Data analysis and visualisation - *Kezia Samuels*

### Load the data

```{r}
#| label: load-env-analysis

# Environmental data
num_env <- read_csv("Project_workplan/KEZIA/Num_env_var.csv")
View(num_env)

# Species abundance data 
adder_abd <- read_csv("Project_workplan/KEZIA/Adder_Spe_ABD_Clean.csv")
View(adder_abd)
```

### Data wrangling

Remove the latitude and longitude columns from the environmental dataset.

Name the rows according to the different cell_IDs.

```{r}
#| label: data-wrangle

env <- num_env %>% 
  select(-c("lat", "lon")) # Remove first two columns (lat and long)
View(env)

env <- env %>% 
  column_to_rownames("cell_ID")
View(env)
```

Rename the rows to the different cell_IDs for the species abundance data

```{r}
#| label: rename-row

adder_abd <- adder_abd %>% 
  column_to_rownames("cell_id")
View(adder_abd)
```

### Separate the environmental dataset

#### Remove these variables from the environmental dataset

```{r}
#| label: rem-env-data

env <- env %>% 
  select(-c("Forested land", "Shrubland", "Grassland", "Waterbodies", "Wetlands", "Barren Land", "Cultivated", "Built-up", "Mines & Quarries"))
View(env)
```

------------------------------------------------------------------------

### Correlation analyses

#### Environmental variables

Calculate the pearson correlation matrix for the environmental variables

```{r}
#| label: corr

env_corr <- round(cor(env), 2)
env_corr
```

Calculate correlation matrix with significance levels

```{r}
#| label: p-values

env_corr_test <- rcorr(as.matrix(env), type = "pearson")

# Extract correlation matrix and p-values
corr_matrix <- env_corr_test$r
p_values <- env_corr_test$P

# Create significance level matrix for display
sig_matrix <- ifelse(p_values < 0.001, "***",
                     ifelse(p_values < 0.01, "**",
                            ifelse(p_values < 0.05, "*", "")))
```

Prepare the data for ggplot

```{r}
#| label: prep-ggplot

corr_df <- melt(corr_matrix)
p_df <- melt(p_values)
sig_df <- melt(sig_matrix)

# Combine data
plot_data <- data.frame(
  Var1 = corr_df$Var1,
  Var2 = corr_df$Var2,
  correlation = corr_df$value,
  p_value = p_df$value,
  significance = sig_df$value
)

# Create labels combining correlation and significance
plot_data$label <- paste0(round(plot_data$correlation, 2), 
                          ifelse(plot_data$significance != "", 
                                 paste0("\n", plot_data$significance), ""))

# Add row and column indices for triangle filtering
plot_data$row_idx <- as.numeric(plot_data$Var1)
plot_data$col_idx <- as.numeric(plot_data$Var2)

# Create upper triangle with diagonal data
plot_data_upper_diag <- plot_data[plot_data$row_idx <= plot_data$col_idx, ]
```

Plot the pearson correlation matrix for the environmental variables

```{r}
#| fig-width: 8
#| fig-height: 8
#| fig.align: center
#| fig.cap: "Figure 1: Correlation matrix of environmental variables. The upper triangle is displayed and shows the Pearson correlation coefficients with significance levels: *** p < 0.001, ** p < 0.01, * p < 0.05. Blue indicates the negative correlations, red indicates the positive correlations. The colour intensity reflects the correlation strength, ranging from -1 to +1."
#| label: plot-corr

# Now your plot code:
ggplot(plot_data_upper_diag, aes(x = Var1, y = Var2, fill = correlation)) +
  geom_tile(color = "white") +
  geom_text(aes(label = label), size = 3) +
  scale_fill_gradient2(low = "steelblue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1, 1), space = "Lab",
                       name = "Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1),
        axis.title = element_blank(),
        plot.title = element_text(hjust = 0.5),
        panel.grid = element_blank()) +
  coord_fixed()
```

------------------------------------------------------------------------

### Variable Inflation Factor (VIF)

Check for multicollinearity using VIF for the environmental variables

First remove the minTemp column as this is very strongly correlated with elevation

```{r}
#| label: VIF-env

mintemp <- env %>% 
  select(-c(MinTemp))

# Check for multicollinearity using VIF
vif_result <- vifstep(mintemp, th = 10)  # threshold of 10 is common, can also use 5

# See which variables are retained
vif_result

# Get the reduced dataset
env_reduced <- exclude(mintemp, vif_result)
```

MeanPrecip and MaxTemp have collinearity issues and are removed from the environmental dataset going forward.

------------------------------------------------------------------------

### PCA

PCA for environmental variables

```{r}
#| label: PCA-env

env_pca <- rda(env_reduced, scale = TRUE) 
env_pca
```

Summary

```{r}
#| label: PCA-summary

summary(env_pca)
```

Extract species scores for the first two PCA axes

```{r}
#| label: PCA-scores

scores(env_pca, display = "species", choices = 1:2)
```

So, the proportion of variation explained by the first PC is:

```{r}
#| label: PCA-prop-1

round(env_pca$CA$eig[1] / sum(env_pca$CA$eig) * 100, 1) # result in %
```

Therefore, the proportion of variation explained by the first two PCs is:

```{r}
#| label: PCA-prop-2

round(env_pca$CA$eig[1:2] / sum(env_pca$CA$eig) * 100, 1) # result in %
```

PC1 + PC2 = 67%

Import the dataset containing the biome information.

Add biome column to the env dataset so that the sites can be colour-coordinated according to their biomes

```{r}
#| label: import-biome

Env_var_clean <- read_csv("Project_workplan/KEZIA/Env_var_clean.csv")
View(Env_var_clean)

# Rename the rows
Env_var_clean <- Env_var_clean %>% 
  column_to_rownames("cell_ID")
View(Env_var_clean)
```

Copy the "biome" column

```{r}
#| label: copy-biome

# Simply copy the biome column from one dataset to the other
env$biome <- Env_var_clean$biome
View(env)
```

Biplots

```{r}
#| fig-width: 8
#| fig-height: 8
#| fig.align: center
#| fig.cap: "Figure 2: Principal Component Analysis (PCA) biplot of environmental variables across South African biomes. The different points within the plot represent the different study sites, all of which are categorised by biome type. The blue vectors represent the direction and magnitude of the different environmental variable species scores, and their contribution to the first two principal components."
#| label: PCA-biplot

# Extract PCA scores and loadings
site_scores <- scores(env_pca, display = "sites", choices = 1:2)
var_scores <- scores(env_pca, display = "species", choices = 1:2)

# Create data frames for plotting
sites_df <- data.frame(
  PC1 = site_scores[, 1],
  PC2 = site_scores[, 2],
  Site = rownames(site_scores),
  Biome = env$biome  
)

variables_df <- data.frame(
  PC1 = var_scores[, 1],
  PC2 = var_scores[, 2],
  Variable = rownames(var_scores)
)

# Scale down the arrows for better visibility
arrow_scale <- 0.2  # Adjust this value to make arrows shorter/longer
variables_df$PC1_scaled <- variables_df$PC1 * arrow_scale
variables_df$PC2_scaled <- variables_df$PC2 * arrow_scale

# Handle missing biomes
sites_df$Biome[is.na(sites_df$Biome)] <- "Unknown"

# Calculate variance explained for axis labels
var_explained <- round(env_pca$CA$eig / sum(env_pca$CA$eig) * 100, 1)

# Create the biplot
ggplot() +
  # Add site points colored by biome
  geom_point(data = sites_df, aes(x = PC1, y = PC2, color = Biome), 
             alpha = 0.6, size = 2.5) +
  
  # Add variable arrows - scaled down
  geom_segment(data = variables_df, 
               aes(x = 0, y = 0, xend = PC1_scaled, yend = PC2_scaled),
               arrow = arrow(length = unit(0.1, "cm")), 
               color = "blue", size = 0.8) +
  
  # Add variable labels with better positioning - using scaled coordinates
  geom_text_repel(data = variables_df, 
                  aes(x = PC1_scaled, y = PC2_scaled, label = Variable),
                  color = "blue", size = 3.5, fontface = "bold",
                  box.padding = 0.5, point.padding = 0.3) +

# Replace your scale_color_manual with:
scale_color_manual(values = qualitative_hcl(11, palette = "Dark 3")) +
  
  # Customize the plot
  labs(x = paste0("PC1 (", var_explained[1], "%)"),
       y = paste0("PC2 (", var_explained[2], "%)"),
       title = "PCA Biplot - Environmental Variables") +
  
  theme_minimal() +
theme(plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
      axis.title = element_text(size = 12),
      axis.text = element_text(size = 10),
      legend.position = "bottom",
      legend.text = element_text(size = 9),
      panel.grid.major = element_blank(),  # Remove major gridlines
      panel.grid.minor = element_blank()) + # Remove minor gridlines
  
  # Add reference lines
geom_hline(yintercept = 0, linetype = "dashed", alpha = 0.5) +
geom_vline(xintercept = 0, linetype = "dashed", alpha = 0.5) 
```

------------------------------------------------------------------------

### CCA

Perform the CCA for the environmental variables

```{r}
#| label: cca-env

cca_result <- cca(adder_abd ~ ., data = env_reduced)
```

Summary

```{r}
#| label: cca-summary

summary(cca_result)
```

Calculate the cumulative proportion of variance explained by the first two canonical axes (CCA1 + CCA2)

```{r}
#| label: cca-prop

# Get eigenvalues
eig_vals <- cca_result$CCA$eig

# Proportion of variance explained by each canonical axis
prop_explained <- eig_vals / sum(eig_vals)

# Cumulative for first two axes
cumulative_CCA1_CCA2 <- sum(prop_explained[1:2])

# Display as percentage
cumulative_percentage <- cumulative_CCA1_CCA2 * 100
print(cumulative_percentage)

```

#### Ordination plots

```{r}
#| fig-width: 8
#| fig-height: 8
#| fig.align: center
#| fig.cap: "Figure 3: Canonical Correspondence Analysis (CCA) biplot illustrating relationships between viper species abundance and environmental variables across South African biomes. The black text represents the viper species, the points represent the different study sites (all of which are categorised by biome type) and the blue vectors represent the respective environmental variables."
#| label: cca-plot

# Get site scores
site_scores2 <- data.frame(scores(cca_result, scaling = 2, display = "sites"))
site_scores2$site <- rownames(site_scores2)
site_scores2$biome <- env$biome[match(site_scores2$site, rownames(env))]

# Get species scores
species_scores <- data.frame(scores(cca_result, scaling = 2, display = "species"))
species_scores$species <- rownames(species_scores)

# Get biplot scores (environmental variables)
biplot_scores <- data.frame(scores(cca_result, scaling = 2, display = "bp"))
biplot_scores$variable <- rownames(biplot_scores)

# SOLUTION 1: Scale up the arrows for better visibility
arrow_scale2 <- 3  # Adjust this multiplier as needed
biplot_scores_scaled <- biplot_scores
biplot_scores_scaled$CCA1 <- biplot_scores$CCA1 * arrow_scale2
biplot_scores_scaled$CCA2 <- biplot_scores$CCA2 * arrow_scale2

# SOLUTION 2: Manual label positioning to fix overlaps
# Start with basic positioning
biplot_scores_scaled$label_x <- biplot_scores_scaled$CCA1 + 
  0.15 * sign(biplot_scores_scaled$CCA1) * (abs(biplot_scores_scaled$CCA1) / max(abs(biplot_scores_scaled$CCA1)))

biplot_scores_scaled$label_y <- biplot_scores_scaled$CCA2 + 
  0.15 * sign(biplot_scores_scaled$CCA2) * (abs(biplot_scores_scaled$CCA2) / max(abs(biplot_scores_scaled$CCA2)))

# Manual adjustments for specific overlapping labels
# Find the elevation variable and adjust its position
elevation_idx <- which(biplot_scores_scaled$variable == "elevation")
if(length(elevation_idx) > 0) {
  biplot_scores_scaled$label_y[elevation_idx] <- biplot_scores_scaled$label_y[elevation_idx] + 0.3
}

# Alternative: Use hjust and vjust based on quadrant
biplot_scores_scaled$hjust <- ifelse(biplot_scores_scaled$CCA1 > 0, 0, 1)
biplot_scores_scaled$vjust <- ifelse(biplot_scores_scaled$CCA2 > 0, 0, 1)

# Get CCA eigenvalues and calculate percentages
eigenvalues <- cca_result$CCA$eig
total_inertia <- sum(eigenvalues)
cca1_percent <- round((eigenvalues[1] / total_inertia) * 100, 1)
cca2_percent <- round((eigenvalues[2] / total_inertia) * 100, 1)

# Create the ggplot
ggplot() +
  # Add sites as circles colored by biome
  geom_point(data = site_scores2, 
             aes(x = CCA1, y = CCA2, color = biome), 
             size = 3, alpha = 0.7) +
  
  # Add species text - back to regular geom_text
  geom_text(data = species_scores, 
            aes(x = CCA1, y = CCA2, label = species), 
            color = "black", size = 4, fontface = "bold") +
  
  # Add environmental variable arrows - SCALED UP
  geom_segment(data = biplot_scores_scaled, 
               aes(x = 0, y = 0, xend = CCA1, yend = CCA2), 
               arrow = arrow(length = unit(0.3, "cm")), 
               color = "blue", size = 0.7) +
  
  # Add environmental variable labels with manual positioning
  geom_text(data = biplot_scores_scaled, 
            aes(x = label_x, y = label_y, label = variable,
                hjust = hjust, vjust = vjust), 
            color = "blue", size = 4, fontface = "bold") +
  
  # Add reference lines
  geom_hline(yintercept = 0, linetype = "dotted", color = "gray50") +
  geom_vline(xintercept = 0, linetype = "dotted", color = "gray50") +
  
  # Customize the plot with percentages in axis labels
  labs(title = "CCA biplot - Scaling 2",
       x = paste0("CCA1 (", cca1_percent, "%)"), 
       y = paste0("CCA2 (", cca2_percent, "%)"),
       color = "Biome") +
  
  # Use a color palette to match PCA plot
scale_color_manual(values = qualitative_hcl(11, palette = "Dark 3")) +
  
  # Clean theme
  theme_minimal() +
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    legend.position = "bottom",
    plot.title = element_text(hjust = 0.5),
    legend.text = element_text(size = 9)
  )
```

------------------------------------------------------------------------
